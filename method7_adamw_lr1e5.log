[pid] 32263
Overriding config with config/train_shakespeare.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-word'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare'
# gradient_accumulation_steps = 1
# gradient_accumulation_steps = 6
gradient_accumulation_steps = 12
batch_size = 12
# block_size = 256 # context of up to 256 previous characters
# block_size = 256 # context of up to 256 previous characters
block_size = 64 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

# learning_rate = 1e-3 # with baby networks can afford to go a bit higher
learning_rate = 1e-4 # with baby networks can afford to go a bit higher

# max_iters = 5000
# lr_decay_iters = 5000 # make equal to max_iters usually

max_iters = 500000
lr_decay_iters = 500000 # make equal to max_iters usually

min_lr = 1e-5 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model
# init_from ='resume'
Overriding: init_from = resume
Overriding: learning_rate = 1e-05
Overriding: method = 7
tokens per iteration will be: 9,216
Resuming training from out-shakespeare-word-GPT_08-adamw-method7
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
GPTConfig(block_size=64, vocab_size=50304, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=False, share_kv=False, optimizer='adamw', method=7)
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
number of parameters: 29.94M
num decayed parameter tensors: 26, with 29,958,144 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using fused AdamW: True
Traceback (most recent call last):
  File "train.py", line 293, in <module>
    losses = estimate_loss()
  File "/root/miniconda3/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "train.py", line 253, in estimate_loss
    logits, _, loss  = model(X, Y, gradient_only = False)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/nanoGPT/model.py", line 1044, in forward
    (logits, lp_internal, lp_external) = self._forward(idx,targets,gradient_only)
  File "/root/nanoGPT/model.py", line 1110, in _forward
    x, lp_internal_diff = block(x,is_sampling=True)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/nanoGPT/model.py", line 761, in forward
    dx,lp = self.attn(self.ln_1(x),is_sampling)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/nanoGPT/model.py", line 824, in forward
    att_dist = torch.distributions.Categorical(logits=att)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributions/categorical.py", line 66, in __init__
    super().__init__(batch_shape, validate_args=validate_args)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributions/distribution.py", line 61, in __init__
    if not valid.all():
KeyboardInterrupt
