[pid] 31625
Overriding config with config/train_shakespeare.py:
# train a miniature character-level shakespeare model
# good for debugging and playing on macbooks and such

out_dir = 'out-shakespeare-word'
eval_interval = 250 # keep frequent because we'll overfit
eval_iters = 200
log_interval = 10 # don't print too too often

# we expect to overfit on this small dataset, so only save when val improves
always_save_checkpoint = False

wandb_log = False # override via command line if you like
wandb_project = 'shakespeare'
wandb_run_name = 'mini-gpt'

dataset = 'shakespeare'
# gradient_accumulation_steps = 1
# gradient_accumulation_steps = 6
gradient_accumulation_steps = 12
batch_size = 12
# block_size = 256 # context of up to 256 previous characters
# block_size = 256 # context of up to 256 previous characters
block_size = 64 # context of up to 256 previous characters

# baby GPT model :)
n_layer = 6
n_head = 6
n_embd = 384
dropout = 0.2

# learning_rate = 1e-3 # with baby networks can afford to go a bit higher
learning_rate = 1e-4 # with baby networks can afford to go a bit higher

# max_iters = 5000
# lr_decay_iters = 5000 # make equal to max_iters usually

max_iters = 500000
lr_decay_iters = 500000 # make equal to max_iters usually

min_lr = 1e-5 # learning_rate / 10 usually
beta2 = 0.99 # make a bit bigger because number of tokens per iter is small

warmup_iters = 100 # not super necessary potentially

# on macbook also add
# device = 'cpu'  # run on cpu only
# compile = False # do not torch compile the model
# init_from ='resume'
Overriding: init_from = scratch
Overriding: method = 7
Overriding: optimizer = rmsprop
Overriding: learning_rate = 0.005
tokens per iteration will be: 9,216
Initializing a new model from scratch
defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)
GPTConfig(block_size=64, vocab_size=50304, n_layer=6, n_head=6, n_embd=384, dropout=0.2, bias=False, share_kv=False, optimizer='rmsprop', method=7)
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0
number of parameters: 29.94M
num decayed parameter tensors: 26, with 29,958,144 parameters
num non-decayed parameter tensors: 13, with 4,992 parameters
using optimizer:RMSprop (
Parameter Group 0
    alpha: 0.99
    centered: False
    differentiable: False
    eps: 1e-08
    foreach: None
    lr: 0.005
    maximize: False
    momentum: 0
    weight_decay: 0.1

Parameter Group 1
    alpha: 0.99
    centered: False
    differentiable: False
    eps: 1e-08
    foreach: None
    lr: 0.005
    maximize: False
    momentum: 0
    weight_decay: 0.0
)
step 0: train loss 125.7599, val loss 125.7472
iter 0: loss 125.7621, loss_eval:125.7308  time 15352.84ms, mfu -100.00%
iter 10: loss 124.3512, loss_eval:124.3291  time 722.59ms, mfu 0.74%
iter 20: loss 123.3735, loss_eval:123.3510  time 721.46ms, mfu 0.74%
iter 30: loss 120.3368, loss_eval:120.3064  time 729.66ms, mfu 0.74%
iter 40: loss 121.8842, loss_eval:121.8550  time 601.98ms, mfu 0.76%
iter 50: loss 120.7223, loss_eval:120.6976  time 587.90ms, mfu 0.77%
iter 60: loss 123.7018, loss_eval:123.6783  time 604.61ms, mfu 0.78%
iter 70: loss 122.8566, loss_eval:122.8218  time 723.41ms, mfu 0.78%
iter 80: loss 120.8586, loss_eval:120.8416  time 731.27ms, mfu 0.77%
iter 90: loss 120.5043, loss_eval:120.4886  time 723.22ms, mfu 0.77%
iter 100: loss 123.7156, loss_eval:123.6763  time 727.63ms, mfu 0.77%
iter 110: loss 121.8475, loss_eval:121.8083  time 727.10ms, mfu 0.76%
iter 120: loss 116.9697, loss_eval:116.9321  time 720.93ms, mfu 0.76%
iter 130: loss 108.6982, loss_eval:108.6457  time 720.42ms, mfu 0.76%
iter 140: loss 108.7900, loss_eval:108.7399  time 741.55ms, mfu 0.76%
iter 150: loss 104.4519, loss_eval:104.4186  time 707.96ms, mfu 0.76%
iter 160: loss 104.3069, loss_eval:104.2260  time 726.79ms, mfu 0.75%
iter 170: loss 103.5374, loss_eval:103.5131  time 723.29ms, mfu 0.75%
iter 180: loss 103.6817, loss_eval:103.6529  time 687.02ms, mfu 0.76%
iter 190: loss 107.5894, loss_eval:107.5080  time 668.09ms, mfu 0.76%
iter 200: loss 106.8953, loss_eval:106.8190  time 747.81ms, mfu 0.76%
iter 210: loss 103.3694, loss_eval:103.3398  time 726.75ms, mfu 0.75%
iter 220: loss 101.0647, loss_eval:101.0322  time 721.86ms, mfu 0.75%
iter 230: loss 99.8677, loss_eval:99.7701  time 752.85ms, mfu 0.75%
iter 240: loss 93.2545, loss_eval:93.2156  time 727.33ms, mfu 0.75%
step 250: train loss 94.9401, val loss 94.8351
saving checkpoint to out-shakespeare-word-GPT_08-rmsprop-method7
iter 250: loss 90.7218, loss_eval:90.6480  time 14990.29ms, mfu 0.68%
iter 260: loss 88.6255, loss_eval:88.5636  time 732.31ms, mfu 0.68%
iter 270: loss 89.6198, loss_eval:89.5752  time 724.03ms, mfu 0.69%
iter 280: loss 86.6609, loss_eval:86.6162  time 726.24ms, mfu 0.69%
iter 290: loss 88.0357, loss_eval:88.0087  time 720.54ms, mfu 0.70%
iter 300: loss 97.1575, loss_eval:96.8054  time 604.96ms, mfu 0.72%
iter 310: loss 96.0118, loss_eval:95.9822  time 605.97ms, mfu 0.73%
iter 320: loss 97.3037, loss_eval:97.2762  time 704.89ms, mfu 0.74%
iter 330: loss 98.1949, loss_eval:98.1632  time 728.38ms, mfu 0.74%
iter 340: loss 97.3907, loss_eval:97.2079  time 726.35ms, mfu 0.74%
iter 350: loss 96.8658, loss_eval:96.8363  time 717.31ms, mfu 0.74%
iter 360: loss 99.9099, loss_eval:99.8502  time 753.03ms, mfu 0.73%
iter 370: loss 109.8602, loss_eval:109.5479  time 726.07ms, mfu 0.74%
iter 380: loss 104.4549, loss_eval:104.4301  time 737.75ms, mfu 0.73%
iter 390: loss 99.8112, loss_eval:99.0449  time 723.05ms, mfu 0.73%
iter 400: loss 104.9800, loss_eval:104.9398  time 724.25ms, mfu 0.74%
iter 410: loss 104.3511, loss_eval:104.1508  time 696.76ms, mfu 0.74%
iter 420: loss 108.9846, loss_eval:108.8640  time 723.48ms, mfu 0.74%
iter 430: loss 107.1640, loss_eval:107.1402  time 724.86ms, mfu 0.74%
iter 440: loss 103.7188, loss_eval:103.6669  time 724.26ms, mfu 0.74%
iter 450: loss 106.2474, loss_eval:106.2190  time 726.01ms, mfu 0.74%
iter 460: loss 104.0176, loss_eval:103.9924  time 708.74ms, mfu 0.74%
iter 470: loss 103.5425, loss_eval:103.2333  time 724.73ms, mfu 0.74%
iter 480: loss 112.4437, loss_eval:112.2387  time 771.94ms, mfu 0.74%
iter 490: loss 106.7757, loss_eval:106.7048  time 697.52ms, mfu 0.74%
step 500: train loss 104.9235, val loss 105.0280
iter 500: loss 107.0080, loss_eval:106.9834  time 14598.70ms, mfu 0.67%
Traceback (most recent call last):
  File "train.py", line 323, in <module>
    logits, loss, loss_eval = model(X, Y, gradient_only = True)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/nanoGPT/model.py", line 1044, in forward
    (logits, lp_internal, lp_external) = self._forward(idx,targets,gradient_only)
  File "/root/nanoGPT/model.py", line 1110, in _forward
    x, lp_internal_diff = block(x,is_sampling=True)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/nanoGPT/model.py", line 761, in forward
    dx,lp = self.attn(self.ln_1(x),is_sampling)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/nanoGPT/model.py", line 830, in forward
    lp = att_dist.log_prob(att_sample).sum((1,))
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributions/categorical.py", line 123, in log_prob
    self._validate_sample(value)
  File "/root/miniconda3/lib/python3.8/site-packages/torch/distributions/distribution.py", line 299, in _validate_sample
    if not valid.all():
KeyboardInterrupt
